{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migración de Packages y Sp Oracle a HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Spark SQL   \n",
    "\n",
    "Con Spark SQL es posible ejecutar sentencias de codigo SQL gracias a su API , permitiendo unificar los datos de diferentes entornos y sistemas de almacenamiento en memoria.\n",
    "\n",
    "### Entorno de trabajo \n",
    "- Windows 10  \n",
    "- Anaconda\n",
    "- Jupyter con kernel Spark  \n",
    "- Python 3\n",
    "- Package : get_saldos_documento\n",
    "\n",
    "#### Bloque de código SQL \n",
    "```sql\n",
    "    SELECT Nvl(SUM(cucovato - cucovaab - cucovare), 0) AS total\n",
    "    FROM   chilquin.cuencobr a\n",
    "    WHERE  a.cucofact = 20031385\n",
    "           AND a.cucosacu IS NOT NULL\n",
    "           AND a.cucovato - a.cucovaab - a.cucovare > 0\n",
    "           AND a.cucoesta = 'F' \n",
    "```\n",
    "#### Bloque de funcion encapsulada\n",
    "```sql\n",
    "SELECT CHQ_PRUEBAS_BIGDATA.obt_saldo(47090846) AS total from dual\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamada al cluster Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession , SQLContext, Row\n",
    "spark = SparkSession.builder.appName(\"Ejemplo Spark SQL con Python\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiedo la consulta y el driver al Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          TOTAL|\n",
      "+---------------+\n",
      "|3456.0000000000|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Código SQL\n",
    "consulta = \"(SELECT nvl(sum(CUCOVATO - CUCOVAAB -CUCOVARE),0) AS TOTAL FROM   \tCHILQUIN.CUENCOBR a WHERE \ta.CUCOFACT = 47090846AND   \ta.CUCOSACU IS NOT NULL AND   \ta.CUCOVATO - a.CUCOVAAB - a.CUCOVARE > 0 AND   \ta.CUCOESTA='F') tabla\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\").options(url = \"jdbc:oracle:thin:@10.135.30.128:1521:SFTU0708\",\n",
    "        dbtable = consulta,\n",
    "        driver = \"oracle.jdbc.OracleDriver\",\n",
    "        user = \"SFINTERFAZ\",\n",
    "        password = \"S1C5E5E7CD09797FBE8EF33AAF2E28\").load()\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|          TOTAL|\n",
      "+---------------+\n",
      "|3456.0000000000|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Función encapsulada\n",
    "consulta = \"(SELECT SFINTERFAZ.CHQ_PRUEBAS_BIGDATA.obt_saldo(47090846) as total from dual ) final \" \n",
    "\n",
    "df = spark.read.format(\"jdbc\").options(url = \"jdbc:oracle:thin:@10.135.30.128:1521:SFTU0708\",\n",
    "        dbtable = consulta,\n",
    "        driver = \"oracle.jdbc.OracleDriver\",\n",
    "        user = \"SFINTERFAZ\",\n",
    "        password = \"S1C5E5E7CD09797FBE8EF33AAF2E28\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código bloque anónimo (no ejecutar)\n",
    "consulta = \"(set serveroutput on declare    factcodi   NUMBER;    saldo NUMBER;begin    factcodi := 47090846;     SFINTERFAZ.CHQ_PRUEBAS_BIGDATA.get_saldos_documento (factcodi, saldo);    dbms_output.put_line('saldo: ' || saldo); end;) tabla\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\").options(url=\"jdbc:oracle:thin:@10.135.30.128:1521:SFTU0708\", \n",
    "      dbtable = consulta , \n",
    "      driver  = \"oracle.jdbc.OracleDriver\" , \n",
    "      user = \"SFINTERFAZ\" ,\n",
    "      password = \"S1C5E5E7CD09797FBE8EF33AAF2E28\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Py4JJavaError: An error occurred while calling o114.load.\n",
    ": java.sql.SQLSyntaxErrorException: ORA-00903: nombre de tabla no válido\n",
    "\n",
    "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:450)\n",
    "\tat oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:399)\n",
    "\tat oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:1017 `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Librería HPL / SQL (HIVE)\n",
    "\n",
    "#### Características clave\n",
    "\n",
    "\n",
    "- Facilita la migración de la lógica empresarial existente a Hadoop.\n",
    "- Entiende las sintaxis y la semántica de casi cualquier dialecto SQL de procedimiento existente.\n",
    "-  Compatible en gran medida con \n",
    "    - Oracle PL / SQL, ANSI / ISO SQL / PSM (IBM DB2, MySQL, Teradata ie)\n",
    "    - PostgreSQL PL / pgSQL (Netezza)\n",
    "    - Transact-SQL (Microsoft SQL Server y Sybase)\n",
    "    - Apache Hive \n",
    "    - Cloudera Impala \n",
    "    - Amazon Redshift\n",
    "    \n",
    "#### Contra \n",
    "<p>It's not included with Hive 1.1.0 (CDH 5's version of Hive), but is downloadable and usable as a separate package. Note that it's helpful for porting, but it's not optimized like Oracle PL/SQL would be when run in an Oracle DB, or like a Spark job would be when run on a cluster.</p>\n",
    "   \n",
    "#### Instalación en sistemas unix\n",
    "```\n",
    "   wget http://www.hplsql.org/downloads/hplsql-0.3.31.tar.gz\n",
    "    gunzip hplsql-0.3.31.tar.gz \n",
    "    tar -xvf hplsql-0.3.31.tar \n",
    "    cd hplsql-0.3.31\n",
    "    export HADOOP_CLASSPATH=`hadoop classpath`\n",
    "    ./hplsql --version\n",
    "    ./hplsql -e \"select * from employees\"\n",
    "```\n",
    "\n",
    "Implementación : http://agnihotripawan.blogspot.com/2017/10/hplsql-procedural-sql-on-hadoop.html\n",
    "\n",
    "Sitio : http://www.hplsql.org/doc#functions\n",
    "\n",
    "\n",
    "##  Oracle Big Data SQL \n",
    "\n",
    "#### Características clave\n",
    "\n",
    "- Consulta perfecta de datos a través de Oracle\n",
    "Base de datos, Hadoop, Kafka y\n",
    "Fuentes NoSQL\n",
    "- Ejecuta todas las consultas de Oracle SQL sin\n",
    "Modificación - Conservación de la aplicación.\n",
    "inversión\n",
    "- Smart Scan en Hadoop, Kafka,\n",
    "NoSQL y espacios de tabla de Oracle\n",
    "mejorar el rendimiento mediante el análisis y\n",
    "filtrado de datos de forma inteligente donde\n",
    "reside\n",
    "- Habilita el acceso a Oracle Database 12c\n",
    "a las principales distribuciones de Hadoop\n",
    "- Características de Oracle Database Security\n",
    "proporcionar control de acceso único a\n",
    "datos sensibles a través de Oracle\n",
    "Base de datos, Hadoop, Kafka y\n",
    "Datos NoSQL\n",
    "-  Copie fácilmente los datos de Oracle\n",
    "\n",
    "\n",
    "Enlace : https://www.oracle.com/database/big-data-sql/index.html\n",
    "\n",
    "Contacto Oracle Oficina Chile : +56 2 2830 7879\n",
    "\n",
    "Información : \n",
    "\n",
    "- https://www.oracle.com/technetwork/database/bigdata-appliance/learnmore/bigdatasqloverview21jan2015-2408000.pdf\n",
    "\n",
    "- https://www.oracle.com/technetwork/database/bigdata-appliance/overview/bigdatasql-datasheet-2934203.pdf\n",
    "\n",
    "\n",
    "Productos Big Data : https://www.oracle.com/cl/big-data/products.html\n",
    "\n",
    "Oracle integrador :\n",
    "http://www.oracle.com/us/products/middleware/data-integration/hadoop/overview/index.html\n",
    "\n",
    "Extraido de Quora : https://www.quora.com/How-do-I-replace-Oracle-PL-SQL-application-with-Big-data-tools-like-Hive-and-Spark\n",
    "\n",
    "## Migracion de datos con Oracle GoldenGate (licencia)\n",
    "\n",
    " \n",
    "#### Características clave:\n",
    "\n",
    "- El movimiento de datos es en tiempo real, reduciendo la latencia.\n",
    "\n",
    "- Solo se mueven las transacciones comprometidas, lo que permite la consistencia y mejora el rendimiento.\n",
    "\n",
    "- Se admiten diferentes versiones y versiones de Oracle Database junto con una amplia gama de bases de datos heterogéneas que se ejecutan en una variedad de sistemas operativos. Puede replicar datos de una base de datos Oracle a una base de datos heterogénea diferente.\n",
    "\n",
    "- Arquitectura simple y fácil configuración.\n",
    "\n",
    "- Alto rendimiento con una sobrecarga mínima en las bases de datos e infraestructura subyacentes.\n",
    "\n",
    "Información : http://www.oracle.com/us/products/middleware/data-integration/goldengate-for-big-data-ds-2415102.pdf\n",
    "\n",
    "Implementaciones : http://www.ateam-oracle.com/oracle-goldengate-big-data-apply-to-apache-hdfs/\n",
    "\n",
    "Descargas : https://www.oracle.com/technetwork/middleware/goldengate/downloads/index.html\n",
    "\n",
    "\n",
    "##  UDF - Funciones definidas por usuario\n",
    "Estas realmente no son SP  pero permiten definir la logica en java.\n",
    "\n",
    "```\n",
    " protected ResultSet fetchDataFromJdbc(String storedProc) {\n",
    "   Connection connection;\n",
    "   CallableStatement statement;\n",
    "\n",
    "   try {\n",
    "     Class.forName(\"my.db.Driver\");\n",
    "\n",
    "     connection = DriverManager.getConnection(\"url\",\"user\",\"pass\");\n",
    "\n",
    "     statement = connection.prepareCall(storedProc);\n",
    "     statement.registerOutParameter(1, \"outValue\");\n",
    "     statement.executeQuery();\n",
    "\n",
    "     return (ResultSet) statement.getObject(1);\n",
    "   } catch (Exception e) {\n",
    "         e.printStackTrace();\n",
    "         log.error(e.getMessage());\n",
    "   }\n",
    "\n",
    " throw new IllegalStateException(\"Results should have returned.\");\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "    from pyspark.sql.types import StringType\n",
    "    from pyspark.sql.functions import udf\n",
    "    F1 = udf(lambda x: '-1' if x in not_found_cat else x, StringType())\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/46735996/convert-oracle-stored-procedure-to-hive-or-spark\n",
    "\n",
    "\n",
    "\n",
    "## Carga desde archivos\n",
    "\n",
    "Esta opcion permite cargar los achivos sql con java y enviarlos a Hive para completar los parametros de entrada. \n",
    "\n",
    "\n",
    "## Uso de Vistas\n",
    "\n",
    "Almacenadas en la BD\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tener en cuenta\n",
    "\n",
    "### Transcribir la lógica de los packages a scala o pyspark: \n",
    "El flujo de código de las operaciones en los RDBMS es muy diferente al flujo de código en Spark, no hay una compatibilidad asegurada en estos términos para Spark SQL.\n",
    "### Diseñar un marco BD:\n",
    "Seguir la misma lógica de ingeniería inversa para proponer un modelo de datos y reproducirlo a través de scripts.\n",
    "### Tipos de datos: \n",
    "Sera necesario hacer transformaciones para trabajar con tipo de datos entre los entornos.\n",
    "### Ambientes de testing: \n",
    "Es importante considerar ciclos de prueba con ayuda de expertos del negocio, para validar los datos finales.\n",
    "### Caos:\n",
    "Abra muchos casos en los cuales no se logrará un resultado exacto entre aun SP y el código Spark. \n",
    "Una razón de esto es que los RDBMS están altamente estructurados y el procesamiento hace que los datos sean útiles. Sin embargo, al ocupar grandes almacenes, esta cualidad no se podría asegurar de la misma manera.\n",
    " \n",
    "Extraído de Quora :https://www.quora.com/How-can-I-translate-PL-SQL-procedures-to-Spark-Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Migración.ipynb to html\n",
      "[NbConvertApp] Writing 270638 bytes to Migración.html\n"
     ]
    }
   ],
   "source": [
    "#Guarda el actual notebook como html\n",
    "!jupyter nbconvert Migración.ipynb --to html --output Migración.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
