{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD \n",
    "\n",
    "Es el tipo de dato basico de Spark y que se encuentra distrubuido en las maquinas del cluster.\n",
    "Colección de elmentos que es tolerante a fallos y es capaz de funcionar en parallelo.\n",
    "Permite paralelizacion.\n",
    "\n",
    "Evaluacion perezosa \n",
    "- Pro : \n",
    "    - Se gana tiempo.\n",
    "    - Mantiene las tranformaciones un DAG , estas se ejecutan con una accion.\n",
    "    - La API proporciona soporte para Java, Scala, Python and R.\n",
    "- Contra : \n",
    "    - Si hay algun error en el proceso , no se visualizará hasta el termino de la ejecución. \n",
    "    - No es compatible con la optimizacin de Spark (Catalyst , Tungsten)\n",
    "\n",
    "# SparkContext ySparkConf\n",
    "\n",
    "\n",
    "- SparkContext:\n",
    "    - Especifica como nos vamos a conectar ala cluster \n",
    "\n",
    "- SparkConf:\n",
    "    - Contiene información de la aplicacion,  número de hilos, la memoria que queramos que consuma , etc.\n",
    "    \n",
    "    \n",
    "### Ejemplo en Scala\n",
    "\n",
    "\n",
    "```scala\n",
    "val cadenas = Array(“Openwebinars”, “bigdata”, “Openwebinars”)\n",
    "val cadenasRDD = sc . parallelize (cadenas)\n",
    "cadenasRDD.collect()\n",
    "val file = sc.textFile(“/home/vmuser/textoRDD”, 6)\n",
    "file.collect()\n",
    "\n",
    "\n",
    "Aplicando un filtro a los datos\n",
    "\n",
    "val filtro = cadenasRDD.filter(line => line.contains(“Openwebinars”))\n",
    "\n",
    "Error camuflado por Evaluación perezosa \n",
    "\n",
    "val fileNotFound = sc.textFile(“/7añljdlsjd/alkls/”, 6)\n",
    "fileNotFound.collect() \"Salida del error\"\n",
    "```\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "Un uso de Spaqk SQL es ejecutar consultas SQL  , pero tambien puede ser usado para leer datos de Hive. \n",
    "Los datos de salida pueden ser datasets o dataframes.\n",
    "\n",
    "\n",
    "# JDBC\n",
    "\n",
    "```\n",
    "# Loading data from a JDBC source\n",
    "df <- read.jdbc(\"jdbc:postgresql:dbserver\", \"schema.tablename\", user = \"username\", password = \"password\")\n",
    "\n",
    "# Saving data to a JDBC source\n",
    "write.jdbc(df, \"jdbc:postgresql:dbserver\", \"schema.tablename\", user = \"username\", password = \"password\")\n",
    "\n",
    "```\n",
    "\n",
    "https://eddjberry.netlify.com/post/2017-12-05-sparkr-vs-sparklyr/\n",
    "\n",
    "\n",
    "## Guardar y cargar\n",
    "```\n",
    "df <- read.df(\"examples/src/main/resources/users.parquet\")\n",
    "write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.parquet\")\n",
    "```\n",
    "\n",
    "\n",
    "## Consultas directas en archivos\n",
    "```\n",
    "df <- sql(\"SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
